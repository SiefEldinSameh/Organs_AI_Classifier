{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "collapsed": true,
        "id": "_NGQ6o-yl_O9",
        "outputId": "f98f88ed-42c9-4d0e-811e-ccb6dd2ec320"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Kaggle library (if not already installed)\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Step 2: Upload the Kaggle API key (kaggle.json)\n",
        "from google.colab import files\n",
        "files.upload()  # Run this and select the kaggle.json file you downloaded from Kaggle\n",
        "\n",
        "# Step 3: Move kaggle.json to the right directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Secure the API key file\n",
        "\n",
        "# Step 4: List and download the dataset\n",
        "# Replace \"dataset-owner/dataset-name\" with your dataset's path on Kaggle\n",
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset -p /content/Brain\n",
        "!kaggle datasets download -d stevenazy/liver-dataset -p /content/Liver\n",
        "!kaggle datasets download -d omkarmanohardalvi/lungs-disease-dataset-4-types -p /content\n",
        "!kaggle datasets download -d antonbudnychuk/hand-xray -p /content/Hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mIu1rA8mmDLq",
        "outputId": "c42fd78b-8d92-4f33-b3bb-241ca6b63386"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths to each downloaded dataset\n",
        "datasets = {\n",
        "    \"Brain\": \"/content/Brain/brain-tumor-mri-dataset.zip\",\n",
        "    \"Liver\": \"/content/Liver/liver-dataset.zip\",\n",
        "    \"Lung\": \"/content/lungs-disease-dataset-4-types.zip\",\n",
        "    \"Hand\": \"/content/Hand/hand-xray.zip\"\n",
        "}\n",
        "\n",
        "# Extract each dataset to its respective folder\n",
        "for organ, path in datasets.items():\n",
        "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "        extraction_path = f\"/content/{organ}\"  # Set an extraction directory for each organ\n",
        "        zip_ref.extractall(extraction_path)\n",
        "        print(f\"{organ} dataset extracted to {extraction_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k_CEP6vDmJuv",
        "outputId": "4525f76b-238f-4b93-a3d1-81ed3c5d035d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "def copy_files_maintaining_structure(src_dir, dest_parent_dir, organ_name):\n",
        "    \"\"\"\n",
        "    Copies files from source directory to destination while flattening the structure\n",
        "    but maintaining unique filenames\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(src_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                # Get the subfolder name (if any) to preserve in filename\n",
        "                rel_path = os.path.relpath(root, src_dir)\n",
        "                if rel_path != '.':\n",
        "                    # Create unique filename including subfolder information\n",
        "                    new_filename = f\"{rel_path.replace(os.sep, '_')}_{file}\"\n",
        "                else:\n",
        "                    new_filename = file\n",
        "\n",
        "                src_path = os.path.join(root, file)\n",
        "                dest_dir = os.path.join(dest_parent_dir, organ_name)\n",
        "                os.makedirs(dest_dir, exist_ok=True)\n",
        "                dest_path = os.path.join(dest_dir, new_filename)\n",
        "                shutil.copy2(src_path, dest_path)\n",
        "\n",
        "# Define paths for each organ\n",
        "base_dir = \"/content\"\n",
        "train_data = {\n",
        "    'Brain': os.path.join(base_dir, \"Brain/Training\"),\n",
        "    'Hand': os.path.join(base_dir, \"Hand/train/hand\"),\n",
        "    'Liver': os.path.join(base_dir, \"Liver/liver/train\"),\n",
        "    'Lung': os.path.join(base_dir, \"Lung/Lung Disease Dataset/train\")\n",
        "}\n",
        "\n",
        "# Image parameters\n",
        "image_size = (128, 128)\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generators with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create a temporary directory structure for the generator\n",
        "import shutil\n",
        "temp_train_dir = os.path.join(base_dir, 'temp_organized_data')\n",
        "if os.path.exists(temp_train_dir):\n",
        "    shutil.rmtree(temp_train_dir)\n",
        "os.makedirs(temp_train_dir, exist_ok=True)\n",
        "\n",
        "# Organize files into the required structure\n",
        "for organ, src_dir in train_data.items():\n",
        "    print(f\"Processing {organ} images...\")\n",
        "    copy_files_maintaining_structure(src_dir, temp_train_dir, organ)\n",
        "\n",
        "# Create generators\n",
        "print(\"Creating data generators...\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    temp_train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    temp_train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        'best_organ_classifier.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Define the CNN model with proper input layer\n",
        "model = Sequential([\n",
        "    Input(shape=(128, 128, 3)),  # Proper input layer\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')  # 4 classes for 4 organs\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=validation_generator,\n",
        "        epochs=10,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during training: {str(e)}\")\n",
        "finally:\n",
        "    # Clean up temporary directory\n",
        "    shutil.rmtree(temp_train_dir)\n",
        "\n",
        "# Save the final model\n",
        "model.save('final_organ_classifier.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "collapsed": true,
        "id": "sXmd46epIJfl",
        "outputId": "d7b51653-a59f-467a-d989-585d287c8496"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Kaggle library (if not already installed)\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Step 2: Upload the Kaggle API key (kaggle.json)\n",
        "from google.colab import files\n",
        "files.upload()  # Run this and select the kaggle.json file you downloaded from Kaggle\n",
        "\n",
        "# Step 3: Move kaggle.json to the right directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Secure the API key file\n",
        "\n",
        "# Step 4: List and download the dataset\n",
        "# Replace \"dataset-owner/dataset-name\" with your dataset's path on Kaggle\n",
        "!kaggle datasets download -d jarvisgroot/brain-tumor-classification-mri-images -p /content/Brain\n",
        "!kaggle datasets download -d priyamsaha17/liver-segmentation-dataset-2 -p /content/Liver\n",
        "!kaggle datasets download -d tawsifurrahman/tuberculosis-tb-chest-xray-dataset -p /content/Lung\n",
        "!kaggle datasets download -d umeradnaan/x-ray-dection -p /content/Hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DG5-lTlsRy_F",
        "outputId": "2cc1c8a9-9cf5-4243-bc74-c1ef14eaf657"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths to each downloaded dataset\n",
        "datasets = {\n",
        "    \"Brain\": \"/content/Brain/brain-tumor-classification-mri-images.zip\",\n",
        "    \"Liver\": \"/content/Liver/liver-segmentation-dataset-2.zip\",\n",
        "    \"Lung\": \"/content/Lung/tuberculosis-tb-chest-xray-dataset.zip\",\n",
        "    \"Hand\": \"/content/Hand/x-ray-dection.zip\"\n",
        "}\n",
        "\n",
        "# Extract each dataset to its respective folder\n",
        "for organ, path in datasets.items():\n",
        "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "        extraction_path = f\"/content/{organ}\"  # Set an extraction directory for each organ\n",
        "        zip_ref.extractall(extraction_path)\n",
        "        print(f\"{organ} dataset extracted to {extraction_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tpcy8d42HZql",
        "outputId": "85c0ff22-dc72-4741-8206-168025bfb94a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import uuid\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "# Define base directory\n",
        "base_dir = \"/content\"\n",
        "\n",
        "# Define paths for each organ's training data\n",
        "train_data = {\n",
        "    'Brain': os.path.join(base_dir, \"Brain/brain_tumor_mri/new_dataset/bt_images\"),\n",
        "    'Hand': os.path.join(base_dir, \"Hand/Bone Fracture Dataset/training\"),\n",
        "    'Liver': os.path.join(base_dir, \"Liver/images/images\"),\n",
        "    'Lung': os.path.join(base_dir, \"Lung/TB_Chest_Radiography_Database\")\n",
        "}\n",
        "\n",
        "def copy_files_with_unique_names(src_dir, dest_parent_dir, organ_name):\n",
        "    \"\"\"\n",
        "    Copies image files from the source directory to the destination\n",
        "    directory using unique filenames to prevent overwriting.\n",
        "    \"\"\"\n",
        "    image_count = 0\n",
        "    skipped_files = 0\n",
        "    per_folder_counts = {}  # Track counts per folder\n",
        "\n",
        "    for root, _, files in os.walk(src_dir):\n",
        "        # Count total files per subfolder for Lung dataset tracking\n",
        "        folder_name = os.path.relpath(root, src_dir)\n",
        "        valid_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "        per_folder_counts[folder_name] = len(valid_files)\n",
        "\n",
        "        for file in valid_files:\n",
        "            # Generate a unique filename to avoid overwriting\n",
        "            unique_filename = f\"{organ_name}_{uuid.uuid4().hex}_{file}\"\n",
        "\n",
        "            src_path = os.path.join(root, file)\n",
        "            dest_dir = os.path.join(dest_parent_dir, organ_name)\n",
        "            os.makedirs(dest_dir, exist_ok=True)\n",
        "            dest_path = os.path.join(dest_dir, unique_filename)\n",
        "\n",
        "            # Copy the file and count successful copies\n",
        "            shutil.copy2(src_path, dest_path)\n",
        "            image_count += 1\n",
        "\n",
        "    print(f\"\\nTotal images copied for {organ_name}: {image_count}, Skipped non-image files: {skipped_files}\")\n",
        "    print(f\"\\nFile count per folder for {organ_name}: {per_folder_counts}\")\n",
        "\n",
        "# Clear the temporary directory before starting\n",
        "temp_train_dir = os.path.join(base_dir, \"temp_train\")\n",
        "if os.path.exists(temp_train_dir):\n",
        "    shutil.rmtree(temp_train_dir)\n",
        "os.makedirs(temp_train_dir, exist_ok=True)\n",
        "\n",
        "# Print and count total images in each source directory before copying\n",
        "for organ, path in train_data.items():\n",
        "    total_files = sum(len(files) for _, _, files in os.walk(path) if any(f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')) for f in files))\n",
        "    print(f\"Starting copy for {organ}. Total files found in source: {total_files}\")\n",
        "    copy_files_with_unique_names(path, temp_train_dir, organ)\n",
        "\n",
        "# Verify and print the total number of images copied for each organ\n",
        "for organ in train_data.keys():\n",
        "    organ_dir = os.path.join(temp_train_dir, organ)\n",
        "    if os.path.exists(organ_dir):\n",
        "        num_images = len([f for f in os.listdir(organ_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
        "        print(f\"Total {organ} images copied to {temp_train_dir}: {num_images}\")\n",
        "    else:\n",
        "        print(f\"No directory found for {organ} in {temp_train_dir}\")\n",
        "\n",
        "# Image parameters\n",
        "image_size = (128, 128)\n",
        "batch_size = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U55Sz-BkT0fq",
        "outputId": "2096565d-e3c5-4159-ac36-a24c55eaaf33"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iG7J_SihIMdU",
        "outputId": "f349bcf1-2e8b-4559-d729-a005e2429c3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # Import EarlyStopping\n",
        "\n",
        "# Step 1: Load the Pre-trained Model\n",
        "model = load_model('/content/best_organ_classifier_copy.h5')  # Replace with your actual model path\n",
        "\n",
        "# Step 2: Freeze Early Layers\n",
        "# Freeze all layers except the last few (depending on your architecture)\n",
        "for layer in model.layers[:-5]:  # Adjust the number of layers to freeze as needed\n",
        "    layer.trainable = False\n",
        "\n",
        "# Step 3: Prepare the Fine-Tuning Dataset\n",
        "# Use ImageDataGenerator for augmenting your dataset to add variability\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Set aside 20% for validation\n",
        ")\n",
        "\n",
        "# Base directory for image data\n",
        "base_dir = \"/content\"\n",
        "\n",
        "# Define paths for each organ's training data\n",
        "train_data = {\n",
        "    'Brain': os.path.join(base_dir, \"Brain/brain_tumor_mri/new_dataset/bt_images\"),\n",
        "    'Hand': os.path.join(base_dir, \"Hand/Bone Fracture Dataset/training\"),\n",
        "    'Liver': os.path.join(base_dir, \"Liver/images/images\"),\n",
        "    'Lung': os.path.join(base_dir, \"Lung/TB_Chest_Radiography_Database\")\n",
        "}\n",
        "\n",
        "# Combine the paths to create a single training directory structure\n",
        "# Create a temporary directory structure for the ImageDataGenerator\n",
        "temp_train_dir = os.path.join(base_dir, \"temp_train\")\n",
        "\n",
        "# Create a directory for each organ in the temporary train directory\n",
        "for organ, path in train_data.items():\n",
        "    os.makedirs(os.path.join(temp_train_dir, organ), exist_ok=True)\n",
        "\n",
        "    # Copy images from each organ's path to the temporary directory\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add other extensions as necessary\n",
        "            src = os.path.join(path, filename)\n",
        "            dst = os.path.join(temp_train_dir, organ, filename)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "# Prepare the train and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    temp_train_dir,\n",
        "    target_size=(128, 128),  # Use the same size as your original training\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    temp_train_dir,  # Same path as train data\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Step 4: Compile the Model with a Lower Learning Rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),  # Use a lower learning rate\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Fine-Tune the Model\n",
        "# Set up EarlyStopping to monitor validation loss\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=5,  # Stop training if no improvement in validation loss for 5 epochs\n",
        "    restore_best_weights=True  # Restore the model weights from the epoch with the best validation loss\n",
        ")\n",
        "\n",
        "# Train on the augmented dataset to help it generalize better\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,  # You can increase the epochs for early stopping to take effect\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping]  # Include the EarlyStopping callback\n",
        ")\n",
        "\n",
        "# Save the model to Google Drive\n",
        "model.save('/content/drive/MyDrive/fine_tuned_model.h5')\n",
        "\n",
        "\n",
        "# Optional: Clean up temporary directory after training\n",
        "shutil.rmtree(temp_train_dir)  # Remove the temporary directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "oWChMYVuwHf3",
        "outputId": "b44edefb-1347-42c8-f90c-e5b89241c381"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_path = '/content/drive/MyDrive/fine_tuned_model.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Define the path to the directory containing test images\n",
        "test_image_dir = '/content/test_images'  # Change this to your test images directory\n",
        "\n",
        "# Image preprocessing parameters\n",
        "image_size = (128, 128)  # Adjust based on model's expected input size\n",
        "\n",
        "# Define a dictionary to map class indices to organ names\n",
        "class_labels = {\n",
        "    0: 'Brain',\n",
        "    1: 'Hand',\n",
        "    2: 'Liver',\n",
        "    3: 'Lung'\n",
        "}\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image to be compatible with the model's input requirements.\n",
        "    \"\"\"\n",
        "    img = image.load_img(img_path, target_size=image_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array /= 255.0  # Scale pixel values\n",
        "    return img_array\n",
        "\n",
        "# Function to predict and display results\n",
        "def predict_and_display_results(model, test_image_dir):\n",
        "    \"\"\"\n",
        "    Loads each image from the test directory, processes it, makes a prediction,\n",
        "    and displays the result along with probabilities for each class.\n",
        "    \"\"\"\n",
        "    for img_file in os.listdir(test_image_dir):\n",
        "        img_path = os.path.join(test_image_dir, img_file)\n",
        "\n",
        "        # Preprocess the image\n",
        "        img_array = load_and_preprocess_image(img_path)\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction = model.predict(img_array)[0]\n",
        "\n",
        "        # Get the predicted class and confidence\n",
        "        predicted_class_index = np.argmax(prediction)\n",
        "        predicted_class_name = class_labels[predicted_class_index]\n",
        "        confidence = prediction[predicted_class_index]\n",
        "\n",
        "        # Display the image\n",
        "        plt.imshow(image.load_img(img_path))\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display the predicted organ with confidence\n",
        "        plt.title(f\"Prediction: {predicted_class_name} (Confidence: {confidence:.2f})\")\n",
        "        plt.show()\n",
        "\n",
        "        # Print probabilities for all classes\n",
        "        print(f\"\\nProbabilities for {img_file}:\")\n",
        "        for i, prob in enumerate(prediction):\n",
        "            print(f\"{class_labels[i]}: {prob:.2%}\")\n",
        "\n",
        "# Run predictions on test images\n",
        "predict_and_display_results(model, test_image_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
